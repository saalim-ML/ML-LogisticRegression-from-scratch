# -*- coding: utf-8 -*-
"""Logistic Regression from scratch in python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fVr-AMPib_1j8OCRaTK9PhhomNpy67hQ
"""

# import numpy library
 import numpy as np

"""Build Logistic Regression"""

class logistic_regression():
  # Learning rate and number of literations(Hyperparametes)
  def __init__(self, learning_rate, no_of_iterations):
    self.learning_rate = learning_rate
    self.no_of_iterations = no_of_iterations

  # Fit function to train the model with dataset
  def fit(self, X, Y):
    self.m, self.n = X.shape # m=data points(no.rows), n=(no.columns)(input features)

    #init weight and bias
    self.w = np.zeros(self.n)
    self.b = 0
    self.X = X
    self.Y = Y

    # implementing Gradient Descent for Optimization

    for i in range(self.no_of_iterations):
      self.update_weight()

  def update_weight(self):
    # Sigmoid function
    Y_hat = 1/(1+np.exp(-(self.X.dot(self.w)+self.b))) # y^=1/1+e-z (wX+b)

    # derivaties function
    dw = (1/self.m)*np.dot(self.X.T, (Y_hat-self.Y)) # 1/m*(y^-y).X
    db = (1/self.m)*np.sum(Y_hat-self.Y) # 1/m*(y^-y)

    # weight & bias using gradeint descent
    self.w = self.w - self.learning_rate*dw
    self.b = self.b - self.learning_rate*db

  #siqmoid equation & Decsion Boundary
  def predict(self, X):
    Y_pred = 1/(1+np.exp(-(X.dot(self.w)+self.b)))
    Y_pred = np.where(Y_pred>0.5, 1, 0)
    return Y_pred

"""import the dependencies"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from warnings import filterwarnings
filterwarnings('ignore')

"""**Data collection and Analysis**

Diabeties Data d(diabeties)

Loading the ddata set to a pandas DataFrame
and printing first 5 rows of the data
"""

ddata=pd.read_csv('/content/diabetes.csv')
ddata.head()

"""number of rows and columns in this data set"""

ddata.shape

"""statistical measure of data"""

ddata.describe()

"""value counts of "Outcome" of ddata"""

ddata["Outcome"].value_counts()
# 0=non-diabetic
# 1=diabetic

ddata.groupby("Outcome").mean()

"""Find missing/null value"""

ddata.isnull().sum()

"""separating the data and labels (features & target)"""

# x,y
features= ddata.drop(columns="Outcome", axis=1)
target= ddata["Outcome"]

print(features)

print(target)

"""Data Standardization"""

scalar = StandardScaler()

scalar.fit(features)

standardized_data=scalar.transform(features)

print(standardized_data)

features=standardized_data
target=ddata["Outcome"]

print(features)
print(target)

"""Train-Test-split"""

x_train,x_test,y_train,y_test=train_test_split(features,target,test_size=0.2,stratify=target,random_state=2)

# find shape of train & test
print(features.shape,x_train.shape,x_test.shape)
print(target.shape,y_train.shape,y_test.shape)

"""Training the model"""

classifier=logistic_regression(learning_rate=0.02,no_of_iterations=1000)

# training the support vector classifier
classifier.fit(x_train,y_train)

"""**Accuracy score**

Accuracy score of the training & test data
"""

x_train_prediction=classifier.predict(x_train)
training_data_accuracy=accuracy_score(x_train_prediction,y_train)

print('Accuracy score is:',training_data_accuracy)

x_test_prediction=classifier.predict(x_test)
test_data_accuracy=accuracy_score(x_test_prediction,y_test)

print('Accuracy score is:',test_data_accuracy)

input_data=(4,84,90,23,56,39.5,0.159,25)

# changing the input data to numpy array
input_data_as_numpy_array=np.asarray(input_data)

# reshape the  array as we are predict
input_data_reshaped=input_data_as_numpy_array.reshape(1,-1)

# standardize the input data
std_data=scalar.transform(input_data_reshaped)
print(std_data)

prediction=classifier.predict(std_data)
print(prediction)

if(prediction[0]==0):
  print('The person is not diabetic')
else:
  print('The person is diabetic')

print(input_data_reshaped)